{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.\n",
    "- can be computationally expensive\n",
    "- This chapter discusses two of the most commonly used resampling methods, *cross-validation* and *bootstrap*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility\n",
    "\n",
    "\n",
    "    The process of evaluating a model's performance is known as *model assessment* whereas the process of selecting the proper level of flexibility for a model is known as *model selection*\n",
    "\n",
    "The bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Cross-validation\n",
    "In statistical learning, we focus on the test error. The test error can be easily calculated if a designated test set is available. Unfortuantely, this is usually not the case.\n",
    "\n",
    "In the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. \n",
    "- Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. Such approacheds are discussed in Chapter 6\n",
    "- In this section, we instead consider a class of methods that estimate the test error rate by *holding out* a subset of the training observations from the fitting process and then applying the statistical learning method to those held out observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 The Validation Set Approach\n",
    "\n",
    "Suppose that we would like to estimate the test error assoiated with fitting a particular statistical learning method on a set of observations.\n",
    "\n",
    "The validateion set approach is a very simple strategy, which involves randomly dividing the available set of observations into two partes, a *training set* and a *validation set*\n",
    "\n",
    "Problems\n",
    "- The validation estimate of the test error rate can be highly variable, depending on which observations are included in the training set.\n",
    "\n",
    "- In the validation approach, only a subset of the observations --those that are included in the training set rather than in the validation set-- are used to fit the model. THis can lead to overestimation of the test error rate. \n",
    "\n",
    ".\n",
    "\n",
    "    We will present *cross-validation* , a refinement of the validation set approach that addresses these two issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Leave-One-Out Cross Validation\n",
    "\n",
    "*Leave-one-out-cross-validation* (LOOCV) is closely related to the validation set approach but it attemps to address that method's drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LOOCV, only one observation is used for the validation set, and the remaining observations make up the training set.\n",
    "Then the estimate for the test MSE is the average of the n test error estimates:\n",
    "\n",
    "$$\n",
    "CV_{(n)}=\\frac{1}{n}\\sum_{i=1}^nMSE_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages over the validation set approach of LOOCV:\n",
    "1. Has far less bias due to high number of observations in training sets\n",
    "2. Tends not to overestimate the test error rate as much as the validation set approach\n",
    "3. The validation approach will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always tield the same results\n",
    "#### Caveats\n",
    "1. expensive to implement since the model has to be fit n times\n",
    "\n",
    "#### Shortcuts for least squares linear or polynomial regression\n",
    "\n",
    "$$\n",
    "CV_{(n)}=\\frac{1}{n}\\sum_{i=1}^n(\\frac{y_i=\\hat{y}_i}{1-h_i})^2\n",
    "$$\n",
    "\n",
    "where $h_i$ is the leverage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 k-fold Cross-Validation\n",
    "An alternative to LOOCV is k-fold CV. \n",
    "#### Steps\n",
    "1. This approach involves randomly dividing the set of observations into k groups, or *folds*, of approximately equal size. \n",
    "2. The first fold is treated as a validation set, and the method is fit on the remaining $k-1$ folds\n",
    "3. The mean squared error, $MSE_1$, is repeated k times; each time, a different group of observations is treated as a validation set\n",
    "4. The k-fold CV estimate is computed by averaging these values\n",
    "$$\n",
    "CV_{(k)}=\\frac{1}{k}\\sum_{i=1}^kMSE_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    " There are some variability in the CV estimates as a result of the variability in how the observations are divided into ten folds but this variability is typically much lower than the variability in the test error estimates that results from the validation set approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data.\n",
    "\n",
    "At other times we are interested only in the location of the minimum point in the estimated test MSE curve.\n",
    "- This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error.\n",
    "- For this purpose, the location of the minimum point in the estimated test MSE curve is important, bot the estimated test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned that k-fold CV with $k<n$ has a computational advantage to LOOCV\n",
    "\n",
    "Putting this aside, a less obvious but potentially more imporant advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off\n",
    "\n",
    "For bias: validation set approach > k-fold CV > LOOCV because of the number of observations in the training set\n",
    "For variance LOOCV > k-fold CV\n",
    "- Why?\n",
    "- Because LOOCV have highly correlated outputs. In contrast, in k-fold CV with $k<n$, the outputs are less correlated. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV\n",
    "\n",
    "#### reccomendation\n",
    "Use k = 5 or k =10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 Cross-Validation on Classification Problems\n",
    "In the classification setting, the LOOCV error rate takes the form\n",
    "$$\n",
    "CV_{(n)}=\\frac{1}{n}\\sum_{i=1}^nErr_i,\n",
    "$$\n",
    "where $Err_i=I(y_i \\neq \\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, the Bayes error rate is unknown. We can use cross-validation in order to make decision on which model to use.\n",
    "\n",
    "Note that, the training error tends to decrease as the flexibility of the model increase. In contrast, the test error displays a characteristic U-shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 The Bootstrap\n",
    "The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.\n",
    "\n",
    "Its power lies in the fact taht it can be easily applied to a wide range of statistical learning methods, uncluding some for which a measure of variability is other wise difficult to obtain and is not automatically output by statistical software.\n",
    "\n",
    "### The example\n",
    "1. Problem: Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of $X$ and $Y$ , respectively, where $X$ and $Y$ are random quantities. We will invest a fraction α of our money in $X$, and will invest the remaining $1 − α$ in $Y$ . Since there is variability associated with the returns on these two assets, we wish to choose α to minimize the total risk, or variance, of our investment. In other words, we want to minimize 4Var(αX + (1 − α)Y )$. \n",
    "\n",
    "2. \n",
    "$$\n",
    "\\alpha=\\frac{\\sigma_Y^2-\\sigma_{XY}}{\\sigma_X^2+\\sigma_Y^2-2\\sigma_{XY}}\n",
    "$$\n",
    "\n",
    "3. Generate 100 paired observations of $X$ and $Y$, and estimating $\\alpha$\n",
    "4. Repeated step 3 for 1000 times.\n",
    "5. Calculate $mean(\\hat{\\alpha})$ and $Var(\\hat{\\alpha})$ to estimate $\\alpha$ and $Var(\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice\n",
    "- The procedure for estimating $Var(\\hat{\\alpha})$ above cannot be applied, because for real data we cannot generate new samples from the original population. \n",
    "\n",
    "- However, the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of $\\hat{\\alpha}$\n",
    "\n",
    ".\n",
    "\n",
    "    Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling with replacement observations from the original data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Lab: Cross-Validation and the Bootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading usual libraries\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "summarize ,\n",
    "poly)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new imports\n",
    " \n",
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "(cross_validate , \n",
    " KFold , \n",
    " ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Thevalidation Set Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto, \n",
    "                                          test_size =196, \n",
    "                                          random_state =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model\n",
    "hp_mm=MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model=sm.OLS(y_train,X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.61661706966987"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid-valid_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMSE(terms, response ,\n",
    "    train , test):\n",
    "    mm = MS(terms)\n",
    "    X_train = mm.fit_transform(train) \n",
    "    y_train = train[response]\n",
    "    X_test = mm.transform(test) \n",
    "    y_test = test[response]\n",
    "    results = sm.OLS(y_train, X_train).fit() \n",
    "    test_pred = results.predict(X_test)\n",
    "    return np.mean((y_test - test_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)], 'mpg',\n",
    "                       Auto_train,\n",
    "                       Auto_valid)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Cross-Validation\n",
    "A problem in the data science worlds:\n",
    "I have a function A, I need task B, so I can compute B(A(D), where D is my data.\n",
    "When A and B don't naturally speak to each other, this requires the use of a *wrapper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.23151351792924"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOOCV\n",
    "hp_model = sklearn_sm(sm.OLS, MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model, X,\n",
    "                            Y,\n",
    "                            cv=Auto.shape[0]) \n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443035, 19.03323998])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1)) \n",
    "    M_CV = cross_validate(M,X,Y,cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score']) \n",
    "cv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848399, 19.13719541])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k-fold CV with k<n\n",
    "cv_error = np.zeros(5) \n",
    "cv = KFold(n_splits=10,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=0) # use same splits for each degree \n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X, \n",
    "                          Y,\n",
    "                          cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using shufflesplit\n",
    "validation = ShuffleSplit(n_splits=1, test_size=196,\n",
    "random_state=0) \n",
    "results = cross_validate(hp_model,Auto.drop(['mpg'], axis=1), Auto['mpg'], cv=validation)\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.802232661034168, 1.4218450941091838)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10, test_size=196,random_state=0) \n",
    "results = cross_validate(hp_model, Auto.drop(['mpg'], axis=1), Auto['mpg'],\n",
    "                         cv=validation)\n",
    "results['test_score'].mean(), results['test_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this standard deviation is not a valid estimate of the sam- pling variability of the mean test score or the individual scores, since the randomly-selected training samples overlap and hence introduce correla- tions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 The Booststrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Can be applied in almost all situations with no complicated mathematical calculations are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57583207459283"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Portfolio = load_data(\"Portfolio\")\n",
    "# This function return estimate alpha for the problem above\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False) \n",
    "    return ((cov_[1,1] - cov_[0,1]) /\n",
    "    (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))\n",
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6074452469619002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0) \n",
    "alpha_func(Portfolio,\n",
    "rng.choice(100, 100,\n",
    "replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_SE(func, D,n=None, B=1000, seed=0):\n",
    "    rng = np.random.default_rng(seed) \n",
    "    first_ , second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index,\n",
    "        n,\n",
    "        replace=True) \n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09118176521277516"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_SE = boot_SE(alpha_func, Portfolio ,B=1000, seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Accuracy of a Liear Regression Model\n",
    "We can use bootstrap method to assess the variability of the coefficient estimates and predictions from a statistical learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def boot_OLS(model_matrix, response, D, idx): \n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_) \n",
    "    return sm.OLS(Y_, X_).fit().params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_func= partial(boot_OLS,MS(['horsepower']),'mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The partial function take a function as an argument and freeze some of the arguments of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.88064456, -0.1567849 ],\n",
       "       [38.73298691, -0.14699495],\n",
       "       [38.31734657, -0.14442683],\n",
       "       [39.91446826, -0.15782234],\n",
       "       [39.43349349, -0.15072702],\n",
       "       [40.36629857, -0.15912217],\n",
       "       [39.62334517, -0.15449117],\n",
       "       [39.0580588 , -0.14952908],\n",
       "       [38.66688437, -0.14521037],\n",
       "       [39.64280792, -0.15555698]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0) \n",
    "np.array([hp_func(Auto,rng.choice(392,\n",
    "                                  392,\n",
    "                                  replace=True)) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.848807\n",
       "horsepower    0.007352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_se = boot_SE(hp_func, Auto ,B=1000,seed=10)\n",
    "hp_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the se of the bootstrap and the se in the model may stem from the fact that our model made several assumption while the bootstrap method does not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('skl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "824b1118aeb34277907cf67e6aa2d279c243d5f30303de89e2239204acf1e71f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
