{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Linear Model Selection and Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we consider some approaches for extending the linear model framework\n",
    "Why might we want to use another fitting procedure instead of least squares?\n",
    "\n",
    "- Prediction Accuracy: Provided that the true relationship is approximately linear, the least squares estimates will have low bias. \n",
    "    - If $n>>p$ -- that is if the number of observation, is much larger than the number of variables, then the least squares estimates tend to also have low variance, and hence will perform well on test observations\n",
    "    - If $n$ is not much larger than $p$, then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions \n",
    "    - If $p>n$, then there is no longer a unique least squares coefficient estimate: there are infinitely many solutions. Each of these least squares solutions gives zero error on the training data, but typically very poor test set performance due to extremely high variance.\n",
    "    - By *constraining* or *shrinking* the estimated coeficients, ưe can òten substantially reduce the variance at the cost of negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Interpretability: It is often the case that not all variables used in a model are in fact associated with the response.\n",
    "    - Including irrelevant variables leads to unnecessary complexity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are many alternatives, both classical and modern, to using least squares to fit. In this chapter, we discuss 3 of them:\n",
    "\n",
    "&emsp;<i>Subset Selection</i>. This approach involves identifying a subset of the p predictors that we believe to related to the response\n",
    "\n",
    "&emsp;<i>Shrinkage</i> also known as *regularization*. This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This help reduce variance\n",
    "    \n",
    "&emsp;<i>Dimension Reduction</i>. This approach involves projecting the p predictors into an M-*dimensional subspace*, where $M<p$. This is achieved by computing M different linear combinations, or projections of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Subset Selection\n",
    "\n",
    "### 6.1.1 Best Subset Selection\n",
    "\n",
    "To perform *best subset selection*, we fit a separate least squares regression for each possible combination of the $p$ predictors.\n",
    "\n",
    "The problem of selecting the best model from among $2^p$ possibilities considered by best subset selection is not trivial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Algorithm 6.1 Best subset selection |\n",
    "| --- |\n",
    "| 1. Let $M_0$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.|\n",
    "|2. For $k = 1,2,...p$: $\\\\$(a) Fit all ${p \\choose k}$ models that contain exatly k predictors $ \\\\$ (b) Pick the best among these ${p \\choose k}$ models and call it $M_k$. Here best is defined as having the smallest RSS, or equivalently largest $R^2$|\n",
    "|3. Select a single best model from among $M_0, . . . , M_p$ using using the prediction error on a validation set, $C_p$ (AIC), BIC, or adjusted R2. Or use the cross-validation method.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same ideas apply to other types of models, such as logistic regression. In the case of logistic regression, instead of ordering models by RSS in Step 2 of Algorithm 6.1, we instead use the deviance, a measure that plays the role of RSS for a broader class of models. The deviance is negative two times the maximized log-likelihood; the smaller the deviance, the better the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are $2^p$ models that involve subsets of $p$ predictors. This is over a million models with p>20.\n",
    "Consequently, there are computational shortcuts -- soc called branch-and-bound techniques-- for eliminating some choices, but these have their limitations as p gets large. They also only work for least squares linear regresison. We present computationally efficient alternatives to best subset selection next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Stepwise Selection\n",
    "\n",
    "Problems with best subset selection:\n",
    "- computational limit\n",
    "- large search space can lead to overfitting and high variance\n",
    "For both of these reasons, *stepwise* methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection\n",
    "\n",
    "#### Forward Stepwise Selection\n",
    "\n",
    "- Computationallly efficient\n",
    "- Begin with the null model\n",
    "- Add predictors to the model, one-at-a-time, until all the predictors are in the model\n",
    "- In particular, at each step, the variable that gives the greatest *additional* improvement to the fit is added to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm 6.2 Forward stepwise selection |\n",
    "| --- |\n",
    "| 1. Let $M_0$ denote the null model, which contains no predictors.|\n",
    "|2. For $k = 0,2,...p-1$: $\\\\$(a) Consider all $p-k$ models that augment the predictors in $M_k$ with one addional predictor $ \\\\$ (b) Pick the best among these $p-k$ models and call it $M_{k+1}$. Here best is defined as having the smallest RSS, or equivalently largest $R^2$|\n",
    "|3. Select a single best model from among $M_0, . . . , M_p$ using using the prediction error on a validation set, $C_p$ (AIC), BIC, or adjusted R2. Or use the cross-validation method.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total models are $1+p(p+1)/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward selection cam be applied even in the high-dimensional setting where $n<p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Stepwise Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm 6.3 Backward stepwise selection |\n",
    "| --- |\n",
    "| 1. Let $M_p$ denote the full model, which contains all $p$ predictors.|\n",
    "|2. For $k = p,...,1$: $\\\\$(a) Consider all $k$ models that contrain all but one of the predictors in $M_k$, for a total of $k-1$ predictors $ \\\\$ (b) Pick the best among these $k$ models and call it $M_{k-1}$. Here best is defined as having the smallest RSS, or equivalently largest $R^2$|\n",
    "|3. Select a single best model from among $M_0, . . . , M_p$ using using the prediction error on a validation set, $C_p$ (AIC), BIC, or adjusted R2. Or use the cross-validation method.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Approaches\n",
    "\n",
    "Hybrid version of backward stepwise and forward stepwise. After adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit. Such an approach attempts to more closely mimin best subset selection while retaining the computational advantages of forward and back ward stepwise selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Choosing the Optimal Model\n",
    "After initial steps of above mentioned methods, we ended up with a set of models, each of which contains a subset of the p predictors. \n",
    "As predictors increase, RSS decrease and $R^2$ increase, therefore, they are not suitable for selecting the best model among a collection of models with different numbers of predictors\n",
    "In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches: \n",
    "1. Making an adjustment to the training error to acocunt for the bias due to overfitting\n",
    "2. Directly estimate the test error, using either a validation set approach or a cross-validation appraoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $C_p$, AIC, BIC, and Adjusted $R^2$\n",
    "- generally, training set MSE is an underestmate of test MSE\n",
    "- In particular, the training error will decrease as more variables are included in the model, but the test error may not\n",
    "- Therefore, training set RSS and training set $R^2$ cannot be used to select from among a set of models with different numbers of variables.\n",
    "\n",
    ".\n",
    "    A number of techniques fro adjusting the training error for the model size are available: $C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted $R^2$\n",
    "\n",
    "1. For a fitted least squares model containing d preictors, the $C_p$\n",
    "$$\n",
    "C_p=\\frac{1}{n}(RSS+2d\\hat{\\sigma}^2)\n",
    "$$\n",
    "essensially, $C_p$ statistic adds a penalty of $2d\\hat{\\sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. When determining which of a set of models is best, we choose the model with the lowest $C_p$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model with Gaussian errors, maximum likelihood and least squares are the same thing. In this case AIC is given by \n",
    "$$\n",
    "AIC=\\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\n",
    "$$\n",
    "Note that this formula omitted irrelevant constants\n",
    "Hence for least squares models, $C_p$ and AIC are proportional to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BIC is derived from a Bayesian point of view, but ends up looking similar to $C_p$ and AIC as well. For the least squares model with d predictors, the BIC is given by:\n",
    "$$\n",
    "BIC=\\frac{1}{n}(RSS+log(n)d\\hat{\\sigma}^2)\n",
    "$$\n",
    "Note that $log(n)>2$ for $n>7$, BIC statistic generally places a heavier penalty on models with man variables, and hence results in the selection of smaller models than $C_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For a least squares model with d variables, the adjusted $R^2$ statistic is calculated as \n",
    "$$\n",
    "\\textit{Adjusted }R^2 =1 -\\frac{RSS/(n-d-1)}{TSS/(n-1)}\n",
    "$$\n",
    "Unline $C_p$, AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted $R^2$ indicates a model with a small test error. \n",
    "\n",
    "The intuition behind the adjusted $R^2$ is that once all of the correct\n",
    "variables have been included in the model, adding additional noise variables\n",
    "will lead to only a very small decrease in RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "$C_p$, AIC, BIC all have rigorous theoretical justifications that rely on asymptotic arguments (scenarios where the sample size n i very large). Despite its populartity, and even though it is quite intuitive, the adjusted $R^2$ is not as well motivated in statistical theory as AIC, BIC, and $C_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation and Cross-Validation\n",
    "Instead of making an adjustment for MSE, we can also directly estimate the test error using validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest.\n",
    "\n",
    "Such approach has an advantage over AIC, BIC, $C_p$, and adjusted $R^2$, in that \n",
    "- It provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model.\n",
    "- It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to estimate the error variance ${\\sigma}^2$\n",
    "\n",
    "Note that when cross-validation is used, the sequence of models $M_k$ in Algorith 6.1-6.3 is determined separately for each training fold, and the validation errors are averaged over all folds for each model size k.\n",
    "\n",
    "    This means, for example with best-subset regression, that Mk, the best subset of size k, can differ across the folds. Once the best size k is chosen, we find the best model of that size on the full data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-standard-error rule. \n",
    "- Claculate the standard error of the estimated test MSE for each model size\n",
    "- Select a model using the for which the estimated test error is within one standard error of the lowest point on the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to subset selection, we can fit a model containing all p predictors using a technique that *constrains* or *regularizes* the coefficient estimates, or equivalently, that *shrinks* the coefficient estimates towards zero. The two best-known techniques for shrinking the regression coefficients towards zero are *ridge regression* and the *lasso*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Ridge Regression\n",
    "Recal the least squares fitting procedure:\n",
    "\n",
    "$$\n",
    "RSS=\n",
    "\\sum_{i=1}^n (y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "*Ridge regression* is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i-\\hat{y}_i)^2 + \\lambda\\sum_{i=1}^p\\beta_j^2=RSS+\\lambda\\sum_{i=1}^p\\beta_j^2\n",
    "$$\n",
    "where $\\lambda\\geq 0$ is a tuning parameter, to be determined separately.\n",
    "The $\\lambda\\sum_{i=1}^p\\beta_j^2$ term called a *shrinkage penalty* is small when $\\beta_1,...,\\beta_p$ are close to zero, and so it has the effect of *shrinking* theestimates of $\\beta_j$ towards zero. \n",
    "The tunning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates. \n",
    "- When $\\lambda=0$, the penalty term has no effect, and ridge regresison will produce the least squares estimates.\n",
    "- As $\\lambda$ approach $\\infty$, the impact of the shrinkage penalty grows.\n",
    "- Unlike least squares, which generates only one set of coefficient estimates, ridge regression will produce a different set of coefficient estimates $\\hat{\\beta}_{\\lambda}^{R}$, for each value of $\\lambda$.\n",
    "\n",
    ".\n",
    "\n",
    "    Selecting a good value of λ is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard least squares coefficient estimates are scale equivariant: multiplying $X_j$ by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of 1/c. In other words, regardless of how the $j$th predictor is scaled, $X_j\\hat{\\beta}_j$ will remain the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In constrast, the ridge regression coefficient estimates can change *substantially* when multiplying a given predictor by a constant. In other words, $X_j\\hat{\\beta}_{j,\\lambda}^R$ will depend not only on the value of $\\lambda$, but also on the scaling of the $j$th predictor. In fact, the value of $X_j\\hat{\\beta}_{j,\\lambda}^R$ may even depend on the scaling of the *other* predictors!\n",
    "\n",
    "    Therefore, it is best to apply ridge regression after stanardizing the predictors\n",
    "\n",
    "using the formula\n",
    "$$\n",
    "\\tilde{x}_{ij}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}\n",
    "$$\n",
    "so that they are all on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does Ridge Regression Improve Over Least Squares?\n",
    "\n",
    "- Rooted in the *bias-variance trade-off*\n",
    "    - As $\\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n",
    "\n",
    "- In general, in situations where the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. This means that a small change in the trainning data can cause a large change in the least squares coefficient estimates.\n",
    "- And if $p>n$, then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance.\n",
    "\n",
    ".\n",
    "\n",
    "    Hence ridge regression workds best in situations where the least squares estimates have high variance\n",
    "\n",
    "    Ridge regression also has substantial computational advantages over best subset selection. In fact, one can show that the computations required to solve *simultaneously for all values of λ*, are almost identical to those for fitting a model using least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 The Lasso\n",
    "\n",
    "#### Disadvantage of Ridge regression\n",
    "- Unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model. \n",
    "\n",
    "- The penalty $\\lambda\\sum\\beta_j^2$ will shrink all the coefficients towards zero, but it will not set any of them exactly to zero (unless $\\lambda=\\infty$)\n",
    "\n",
    "- This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large.\n",
    "\n",
    ".\n",
    "    The lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, $\\beta_{\\lambda}^L$, minimize the quantity\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i-\\hat{y}_i)^2 + \\lambda\\sum_{i=1}^p|\\beta_j|=RSS+\\lambda\\sum_{i=1}^p|\\beta_j|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistical parlance, the lasso uses an $l_1$ penalty instead of an $l_2$ penalty. The $l_1$ norm of a coefficient vector $\\beta$ is given by $\\sum_{i=1}^p|\\beta_j|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the $l_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\\lambda$ is sufficiently large. \n",
    "\n",
    "    Hence, much like best subset selection, the lasso performs variable selection\n",
    "\n",
    "    As a result, models generated from the lasso are generally much easier to interpret than those produced from the lasso are generally much easier to interpret than those produced by ridge regresison\n",
    "\n",
    "    We say that the lasso yields sparse models -- that is, models that involve only a subset of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another formulation for Ridge Regression and the Lass0\n",
    "\n",
    "$$\n",
    "\\textit{minimize }\\beta \\{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\\} \\textit{ subject to } \\sum_{j=1}^p|\\beta_j|\\leq s\n",
    "$$\n",
    "for ridge and \n",
    "\n",
    "$$\n",
    "\\textit{minimize }\\beta \\{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\\} \\textit{ subject to } \\sum_{j=1}^p\\beta_j^2\\leq s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When p=2, then the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by $|\\beta_1|+ |\\beta_1| \\leq s$. Similarly, the ridge regresison estimates have the smallest RSS out of all points that lie within the circle defined by $\\beta_1^2+ \\beta_1^2 \\leq s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a formula can also be created for best subset selection: \n",
    "\n",
    "$$\n",
    "\\textit{minimize }\\beta \\{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\\} \\textit{ subject to } \\sum_{j=1}^pI(\\beta_j\\neq 0)\\leq s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Variable Selection Property of the Lasso\n",
    "\n",
    "Since the boundaries of ridge is usually smooth while lasso usually have corners, the contour of $\\hat{\\beta}$ usually touch the region of ridgeo at a smooth point while it touch the lasso at the corner making the coefficient equal to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the Lasso and Ridge Regresison\n",
    "\n",
    "- Lasso produces simpler and more interpretable models that involve only a subset of the predictors.\n",
    "- While the bias of the two methods are similar, the variance of ridge is slightly lower than lasso if most of the predictors are related to the response\n",
    "- If a few predictors are related to the response, the lasso tends to outperform ridge regression in terms of bias, variance, and MSE\n",
    "- Neither Ridge or Lasso wiil universally dominate the other. Cross-validation can be used in order to determine which approach is better on a particular data set\n",
    "\n",
    ".\n",
    "\n",
    "     Note that there are very efficient algoriths for fitting both ridge and lasso models; in both cases the entire coefficient paths can be computed with about the same amount of work as a signle least squarees fit.\n",
    "\n",
    "The type of shrinkage performed by the lasso is usually known as soft-thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Interpreation of Ridge Regression and the Lasso\n",
    "I should read more about Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Selecting the Tuning Parameter\n",
    "\n",
    "Using cross-validation\n",
    "- if $\\lambda$ for ridge is small and the dip of error is not very pronounced, might consider use least square\n",
    "- If only a few predictors are related to the response, lasso will outperform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Dimension Reduction Methods\n",
    "\n",
    "Transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as *dimension reduction* methods\n",
    "\n",
    "Let $Z_1,...,Z_m$ represent $M<p$ *linear combinations* of our original $p$ predictors that is\n",
    "$$\n",
    "Z_m=\\sum_{j=1}^p\\phi_{jm}X_j\n",
    "$$\n",
    "for some constants $\\phi_{1m},...,\\phi_{pm}, m=1,...,M.$ We can then fit the linear regression model\n",
    "\n",
    "$$\n",
    "y_i=\\theta_0+ \\sum_{m=1}^M\\theta_m z_{im}+\\epsilon_i \\textit{ for }i=1,...,n\n",
    "$$\n",
    "using least squares\n",
    "If the constants $\\phi_{1m},...,\\phi_{pm}, m=1,...,M.$ are chosen wisely, such dimension reduction approaches can often outperform least squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term *dimension reduction* comes from the fact that this approach reduces the problem of estimating the p+1 coefficients to the M+1 coefficients\n",
    "where\n",
    "$$\n",
    "\\beta_j=\\sum_{m=1}^M\\theta_m\\phi{jm}\n",
    "$$\n",
    "\n",
    "Dimension reduction serves to constrain the estimated $\\beta_j$ since now they must take a linear form of products of $\\theta$ and $\\phi$\n",
    "This approach increase the bias but in situations where p is large relative to n, this approach reduce significantly the variance of the fitted coefficients. If M=p, ad all the $Z_m$ are linearly independent, then there is actually no constranints and no dimension reduction occurs.\n",
    "\n",
    "All dimension reduction methods work in two steps. First, the transformed predictors $Z_1,...,Z_M$ are obtatined. Second, the model is fit using these M predictors. However, the choice of $Z_1,...,Z_M$ can be achieved in different ways. For this chapter, we talk about principal components and partial least squares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Principal Components Regression\n",
    "\n",
    "*Principal components analysis* (PCA) is a popular approach for deriving a low-dimensional set of features from a large set of variables. It is discussed in greater detail as a tool for unsupervised learning in Chapter 12. \n",
    "Here we describe its use as a dimension reduction technique for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Overview of Principal Components Analysis\n",
    "The first principal component direction of the data is that along which the observations vary the most. That is, if we *projected* the observations on to the dimension of the first principal component, then the resulting projected observations would have the largest possible variance\n",
    "\n",
    "$$\n",
    "Z_1=\\phi_{11}\\times (X_1-\\bar{X}_1)+ \\phi_{12}\\times (X_2-\\bar{X}_2)\n",
    "$$\n",
    "\n",
    "The idea is that out of every possible *linear combination* of $X_1$ and $X_2$ such that $\\phi_{11}^2+\\phi_{21}^2=1$, the particular linear combination yields the highest variance: i.e. this is the linear combination for which $Var(\\phi_{11}\\times (X_1-\\bar{X}_1)+ \\phi_{12}\\times (X_2-\\bar{X}_2))$ is maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to consider only linear combinations of the form $\\phi_{11}^2+\\phi_{21}^2=1$, since otherwise we could increase $\\phi_{11}$ and $\\phi_{21}$ arbitrarily in order to blow up the variance.\n",
    "The value of $z_{11},...,z_{n1}$ are known as the principal component scores.\n",
    "\n",
    "The first principal component vector defines the line that is as close as possible to the data since the first principal component has been chosen so that the projected observations are as close as possible to the original observations\n",
    "\n",
    "So far we have concentrated on the first principal component. In general, once can construct up to p distinct principal components. The second principal component $Z_2$ is a linear combination of the variables that is uncorrelated with $Z_1$, and has largest variance subject to this constraint. It turns out the the zero correlation condition is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Principal Components Regression Approach\n",
    "\n",
    "The *principal components regression* (PCR) approach involves constructing the first M principal components $Z_1,...,Z_M$, and then using these components as the predictors in a linear regression model that is fit using least squares.\n",
    "\n",
    "The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as weel as the relationship with the response. <b>In other woords, we assume that the directions in which $X_1,...,X_p$ show the most variation are the directions that are associated with Y<\\b>\n",
    "\n",
    "While this assumption is not guaranteed to be true, it often turns out to be a reasonable enough approximation to give good results.\n",
    "\n",
    "If the assumption underlying PCR holds, then fitting a least squares model to $Z_1,...,Z_M$, will lead to better results than fitting a least squares model to $X_1,...,X_p$, since most or all of the information in the data that relates to the response is contained in $Z_1,...,Z_M$, and by estimating only $M<<p$ coefficients we can mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    PCR is not a feature selection method. This is because each of the M principal components used in the regression is a linear combination of all p of the original features. In this sense, PCR is more closely related to ridge regresison thant o the lasso. \n",
    "\n",
    "    Should standardizing each predictor before generating the principal components. In the absence of standardiztion, the high-variance variables will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model. However, if the variables are all measured in the same units (say kilograms), then one might choose not to standardize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Partial Least Squares\n",
    "\n",
    "In PCR, the directions are identified in an *unsupervised* way since the response $Y$ is not used to help determine the principle component directions. That is the response does not supervise the identification of the principal components. Consequently, PCR sufferes from a drawback: there is no guarantee that the directions that best explain the predictors will also ve the best directions to use for predicting the response\n",
    "\n",
    "We now consider *partial least squares* (PLS), a <b>supervised alternative</b> to PCR. \n",
    "\n",
    "    Like PCR, we identifies a new set of M features that are linear combinations of the original features, and then fits a linear model via least squares using these M new features.\n",
    "\n",
    "    Unlike PCR, identifies these new features in a supervised way-that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of PLS\n",
    "1. standardize p predictors\n",
    "2. PLS computes the first direction $Z_1$ by setting each $\\phi_{j1}$ equal to the coefficient from the <b>simple linear regression</b> of $Y$ onto $X_j$\n",
    "\n",
    "Computing $Z_1=\\sum_{j=1}^p\\phi_{j1}X_j$, PLS places the highest weight on the variables that are most strongly related to the response\n",
    "\n",
    "To identify the second PLS direction we first adjust each of the variables for $Z_1$, by regressing each variable on $Z_1$ and taking residuals. These residuals can be interpreted as the remaing information that ahs not been explained by the first PLD direction. \n",
    "4. We then compute $Z_2$ using this orthogonalized data in exactly the same fashion as $Z_1$ was computed based on the original data.\n",
    ".\n",
    ".\n",
    ".\n",
    "5. Finally, we use least squares to fit a linear model to predict Y using $Z_1,...,Z_M$\n",
    "\n",
    "The number M is a tuning parameter that is typically chosen by cross-calidation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Considerations in High Dimensions\n",
    "\n",
    "### 6.4.1 High-Dimensional Data\n",
    "Most traditional statistical techniques for regression and classification are intended for the low-dimensional setting\n",
    "\n",
    "Data sets containing more features than observations are often referred to as *high-dimensional*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 What Does Wrong in High Dimensions?\n",
    "When $p>n$, least squares will always yield a set of coefficient estimates that result in a perfect fit to the data regardless of whether or not there truly is a relationship between the features and the response. Such perfit fit will always lead to overfitting.\n",
    "\n",
    "When $p>n$ or $p\\sim n$, a simple least squares regression line is too flexible and hence overfits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual methods for evaluating models ($C_p$, AIC, BIC) are not appropriate in the high-dimensional setting because estimating $\\hat{\\sigma}^2$ = 0 in high-dimensional. Same for adjusted $R^2$ since it is easy to obtain a model with adjusted $R^2=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 Regression in high Dimensions\n",
    "Less flexible least squares models such as forward stepwise selection, ridge regression, the lasso and principal components regression are particularly useful in this scenario.\n",
    "\n",
    "#### 3 important comments\n",
    "1. Regularization or shrinkage plays a key role in high-dimensioanl problems\n",
    "2. Appropriate tuning parameter selection is crucial for good predictive performance\n",
    "3. The test error tends to increase as the dimensionality of the problem increase, unless the additional features are truly associated with the response\n",
    "\n",
    "    The third point is a key principle in the analysis of high-dimensional data, which is known as the <b>curse of dimensionality</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In general, adding additional signal features that are truly associated with the response will improve the fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model, and consequently an increased test set error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Even if the new features are relevant, the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.4 Interpreting Results in High Dimensions\n",
    "\n",
    "    In the high-dimensional setting, the multicolinearity problem is extreme\n",
    "\n",
    "    \n",
    "    This means that we can never known exactly which variables (if any) truly are predictive of the outcom, and we can never identify the best coefficients for use in the regresison. At most, we can hope to assign large regression coefficients to variables that are correlated with the variables that truly are predictive of the out come\n",
    "\n",
    "Example: suppose we try to predict blood pressure on the basis of half a million SNPs, using forward stepwise selection indicates that 17 of those SNPs lead to a good predictive model on th etrianing data. It wouls be incorrect to consluce that these 17 SNPs predict blood pressure more effectively than the other SNPs not included in the model. There are likely to be many sets of 17 SNPs that would predict blood pressure just as well as the selected model. If we were to obtain an independent data set and perform forward stepwise selection on that data set, we would likely obtain a model containing a different, and perhaps even non-overlapping, set of SNPs. This does not detract from the value of the model obtained— for instance, the model might turn out to be very effective in predicting blood pressure on an independent set of patients, and might be clinically useful for physicians. But we must be careful not to overstate the results obtained, <b>and to make it clear that what we have identified is simply *one of many possible models for predicting blood pressure*, and that it must be further validated on independent data sets</b>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
