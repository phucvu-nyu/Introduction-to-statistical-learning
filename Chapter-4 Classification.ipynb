{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Classification\n",
    "- Logistic regression\n",
    "- Linear discriminant analysis\n",
    "- Quadratic discriminant analysis\n",
    "- Naive Bayes\n",
    "- K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 An Overview of Classification\n",
    "Happen when $Y$ is categorical\n",
    "## 4.2 Why Not Linear Regression\n",
    "Because coding $Y$ (at least 3 categories) as numbers such as $(1,2,3)$ implies that there is a middle value and the distances between the middle value to the other two are the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about binary $Y$? We can and turn out we will have a same result as we use *linear discriminant analysis* (LDA). However, the value can exceed 0.5 and it is not meaningful for interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic Regression\n",
    "- if we use $Pr(Y)$ as $Y$, then we can have negative probabilities\n",
    "### 4.3.1 The Logistic Model\n",
    "We use a function that gives outputs between 0 and 1 for all values of $X$. \n",
    "Many functions meet this description. In logistic regression, we use the *logistic function*\n",
    "$$\n",
    "p(X)=\\frac{e^{\\beta_0+\\beta_1X}}{1+ e^{\\beta_0+\\beta_1X}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model, we use a method called maximum likelihood.\n",
    "With manipulation we have:\n",
    "$$\n",
    "log(\\frac{p(X}{1-p(X)})=\\beta_0+\\beta_1X\n",
    "$$\n",
    "The left-hand side is called the *log odds* or *logit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Estimating the Regression Coefficients\n",
    "Intuition behind maximum likelihood:\n",
    "1. Seek estimates for $\\beta_0$ and $\\beta_1$ such that the predicted probability $\\hat{p}(x_i)$ of $Y$ for each observation corresponds as closely as possible to $y_i$. This can be formalized using the likelihood function:\n",
    "$$\n",
    "l(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i':y_{i'}=0}(1-p(x_{i'}))\n",
    "$$\n",
    "2. The estimates $\\hat{\\beta_0}$ and $\\hat{\\beta_01}$ are chosen to maximize this likelihood function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Making predictions\n",
    "### 4.3.2 Multiple Logistic Regression\n",
    "### 4.3.2 Multinomial Logistic Regression\n",
    "- More than two classes\n",
    "- First, we need to choose a single class to serve as the baseline; without loss of generality, we select the Kth class for this role. Then we have a model:\n",
    "$$\n",
    "Pr(Y=k|X=x)=\\frac{e^{\\beta_{k0}+ \\sum_{i=1}^p\\beta_{ki}x_i}}{1+ \\sum_{l=1}^{K-1}e^{\\beta_{l0}+ \\sum_{i=1}^p\\beta_{li}x_i}}\n",
    "$$\n",
    "for k = 1,...,K-1, and\n",
    "$$\n",
    "Pr(Y=K|X=x)=\\frac{1}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+ \\sum_{i=1}^p\\beta_{li}x_i}}\n",
    "$$\n",
    "From which we have:\n",
    "$$\n",
    "log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})=\\beta_{k0}+ \\sum_{i=1}^p\\beta_{ki}x_i\n",
    "$$\n",
    "The choice of K is not important. Different K will result in different coefficients but the predicted probabiity is not changed\n",
    "- An alternative coding for multinomial logistic regression, known as the softmax coding. It is equivalent to the coding just described in the sense that the fitted values, log odds, between any pair of classes, and other key model outputs will remain the same\n",
    "\n",
    "$$\n",
    "Pr(Y=k|X=x)=\\frac{e^{\\beta_{k0}+ \\sum_{i=1}^p\\beta_{ki}x_i}}{\\sum_{l=1}^{K}e^{\\beta_{l0}+ \\sum_{i=1}^p\\beta_{li}x_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Generative Models for Classification\n",
    "- Logistic regression involves directly modeling $Pr(Y=k|X=x)$ using the logistic function, given by for the case of two response classes or we model the conditional distribution of the response $Y$, given the predictors $X$.\n",
    "- Now we consider an alternative and less direct approach to estimating these probabilities. \n",
    "    - In this approach, we model the distribution of predictors X separately in each of the response classes\n",
    "    - Then we use Bayes' theorem to flip these around into estimates for $Pr(Y=k|X=x)$\n",
    "    - When the distribution of $X$ within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.\n",
    "- Reason for another method:\n",
    "    - When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.\n",
    "    - If the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression\n",
    "    - The methods in this section can be naturally extended to the case of more than two response classes. \n",
    "\n",
    "Let $f_k(X)= Pr(X|Y=k)$ and $\\pi_k=Pr(Y=k)$, then using Bayes's theorem, we have:\n",
    "$$\n",
    "Pr(Y=k|X=x)= \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation: $p_k(x)=Pr(Y=k|X=x)$; this is the posterior probability that an observation $X=x$ belongs to the kth class.\n",
    "Technically, this definition is only correct if $X$ is a qualitative random variable. If $X$ is quatitative, then $f_k(x)dx$ corresponds to the probability of $X$ falling in a samll region $dx$ around x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly computing the posterior probability $p_k(x)$, we can simply plug in estimates of $\\pi_k$ and $f_k(x)$.\n",
    "$\\pi_k$ is easy to estimate but not $f_k(x)$. Therefore, we use different methods to estimate $f_k(x)$. Below would be three different estimates of $f_k(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Linear Disciminant Analysis for p =1\n",
    "$f_k(x)$ is assumed to be *normal* or *Gaussian*\n",
    "In the one-dimensional setting (p=1), the normal density takes the form\n",
    "$$\n",
    "f_k(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2{\\sigma_k}^2}(x-\\mu_k^2))\n",
    "$$\n",
    "where $\\mu_k$ and $\\sigma^2_k$ are the mean and variance parameter for the kth class. For now, let further assume equal variance (i.e. $\\sigma _1^2=...=\\sigma_K^2$)\n",
    "Replacing these assumption into Bayes theorem, we have:\n",
    "$$\n",
    "Pr(Y=k|X=x)= \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2{\\sigma}^2}(x-\\mu_k^2))}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2{\\sigma}^2}(x-\\mu_l^2))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to assigning the observation to the class for which \n",
    "$$\n",
    "\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)\n",
    "$$\n",
    "is largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, even if we are quite certain of our assumption that $X$ is\n",
    "drawn from a Gaussian distribution within each class, to apply the Bayes\n",
    "classifier we still have to estimate the parameters $\\mu_1,...,\\mu_K,\\pi_1,...,\\pi_K$ and $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *linear discriminant analysis* (LDA) method approximates the Bayes classifier by plugging estimates for $\\pi_k,\\mu_k,$ and $\\sigma^2$ using:\n",
    "$$\n",
    "\\hat{\\mu}_k=\\frac{1}{n_k}\\sum_{i:y_i=k}x_i\\\\\n",
    "\\sigma^2 = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k^2)\\\\\n",
    "\\hat{\\pi}_k = n_k/n\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world linear stems from the fact that the discriminant functions\n",
    "\n",
    "$$\n",
    "\\hat{\\delta}_k(x)=x\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+log(\\hat{\\pi}_k)\n",
    "$$\n",
    "are linear functions of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Linear Discriminant Analysis for p>1\n",
    "We now extend the LDA classifier to the case of multiple predictors. For this, we asusme that $X=(X_1,...,X_p)$ is drawn from a* multivariate Gaussian* distribution with a class-specific mean vector and a common covariance matrix.\n",
    "#### Review of multivariate Gaussian distribution\n",
    "- Assumes that ach individual predictor follows a one-dimensional normal distribution\n",
    "- Some correlation between each pair of predictors\n",
    "- Write as $X\\sim N (\\mu,\\mathbf{\\Sigma})$\n",
    "- Formally, the multivariate Gaussian density is defined as \n",
    "$$\n",
    "f(x)= \\frac{1}{(2\\pi)^{p/2}|{\\mathbf{\\Sigma}}|^{1/2}}exp(-\\frac{1}{2}(x-\\mu)^T\\mathbf{\\Sigma}^{-1}(x-\\mu))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug these in the Bayes theorem with some algebra, this is equivalent to assigning the observation to the class for which :\n",
    "$$\n",
    "\\delta_k(x)=x^T\\mathbf{\\Sigma^{-1}}\\mu_k-\\frac{1}{2}\\mu_k^T\\mathbf{\\Sigma}^{-1}\\mu_k+log(\\pi_k)\n",
    "$$\n",
    "note that $\\delta(x)$ is a linear function of x and LDA rule depends on x only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caveats of LDA\n",
    "- First of all, training error rates will usually be lower than test error\n",
    "rates, which are the real quantity of interest. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role\n",
    "- Second, disproportions between $\\pi_k$ among classes can lead to a small error rate with the trivial null classifier. \n",
    "    - Can use confusion matrix for class-specific performance\n",
    "    - in medical term: sensitivity and specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why LDA may not be a good fit for class-specific performance?\n",
    "- LDA is trying to approximate the Bayes classifier, which has the lowest *total* error rate out of all classifiers.\n",
    "    - Hence, class-specific performance may not be a good fit\n",
    "How can we improve?\n",
    "- Note that Bayes classifier and LDA uses a threshold of 0.5 for te posterior probability.\n",
    "    - Therefore, if we are concerened about a specific class, then we can consider lowering or increasing this threshold\n",
    "    - Decision on which threshold value is best must be based on domain knowledge, such as detailed information about the costs associated.\n",
    "- The ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. *short for receiver operating characteristics*\n",
    "    - X axis: 1- specificity\n",
    "    - Y axis: sensitivity\n",
    "    - The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC)\n",
    "\n",
    "| Name | Definition | Synonyms |\n",
    "| --- | --- | --- |\n",
    "| False Positive rate | FP/N | Type I error, 1-Specificity |\n",
    "| True Positive rate | TP/P | 1- Type II error, power, sensitivity, recall |\n",
    "| Positive Prediction value | TP/P* | Precision, 1-false discovery proportion |\n",
    "| Negative Prediction value | TN/N* |  |\n",
    "\n",
    "\n",
    "With N,P being true Negative and Positive while N*, P* being number of nagative and positive prediction respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Quadratic Discriminant Analysis\n",
    "Similar with LDA, QDA assumes the observations from each class are drwan from a Gaussian distribution\n",
    "Unline LDA, QDA assumes that each class has its own covariance matrix. That is,it assumes that an observation from the kth class is of the form $X ∼ N(μk,Σk)$, where $Σ_k$ is a covariance matrix for the kth class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why does is matter whether or not we asusme that the K classes share a common covariance matrix? In other words, why would one prefer LDA to QDA, or vice-versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The bias-variance trade-off.\n",
    "- When there are p predictors, estimating a covariance matrix requires estimating p(p+1)/2 parameters for LDA and Kp(p_1)/2 parameters for QDA\n",
    "    - Consequently, QDA is more flexible classifier than LDA, and so has substantially higher variance. Which can lead to lower prediction performance.\n",
    "    - However, if LDA's assumtion that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias\n",
    "- Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial.\n",
    "- In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Naive bayes\n",
    "Instead of make the assumption of the form of $f_k(x)$, Naive Bayes model assume that \n",
    "<h1><center>\n",
    "within the kth class, the p predictors are independent\n",
    "</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stated mathematically, this assumption means that for $k=1,...,K$,\n",
    "$$\n",
    "f_k(x)= f_{k1}(x_1)\\times ...\\times f_{kp}(x_p)\n",
    "$$\n",
    "where $f_{kj}$ is the density function of the $j$th predictor among obserations in the $k$th class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is this assumption so powerful? \n",
    "- Essentially, estimating a p-dimensional density function is challenging because we must consider not only the marginal distribution of each predictor — that is, the distribution of each predictor on its own— but also thejoint distribution of the predictors — that is, the association between the different predictors.\n",
    "- Do we really believe the naive Bayes?\n",
    "    - Nah ah, but most of the time, it often leads to pretty decent results, especially in settings where n is not large enough relative to p for us to effectively estimate the joint distribution of the predictors within each class.\n",
    "    - It is also very convenient\n",
    "    - In fact, since estimating a joint distribution requires such a huge amount of data, naive Bayes is a good choice in a wide range of settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of Naive Bayes model\n",
    "\n",
    "Once we have made the naive Bayes assumption, we have:\n",
    "\n",
    "$$\n",
    "Pr(Y=k|X=x)= \\frac{\\pi_kf_{k1}(x_1)...f_{kp}(x_p)}{\\sum_{l=1}^K\\pi_lf_{l1}(x_1)...f_{lp}(x_p)}\n",
    "$$\n",
    "\n",
    "If $X_j$ is quatitative, then we can assume that $X_j|Y=k \\sim N(\\mu_{jk},\\sigma_{jk}^2)$\n",
    "\n",
    "If $X_j$ is quantitative, use a non=parametric estimate for $f_{kj}$.\n",
    "\n",
    "    - We can estimate $f_{kj}(x_j) using the fraction of the training observations in the kth class$\n",
    "    \n",
    "    - Also kernel density estimator, which essensitally a smoothed version of a histogram\n",
    "\n",
    "If $X_j$ is qualitative, then we can simply count the proportion of training observations for the jth predictor corresonding to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 A comparison of Classification Methods\n",
    "### 4.5.1 An analytical comparison\n",
    "#### LDA\n",
    "\n",
    "$$\n",
    "log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= log(\\frac{\\pi_k}{\\pi_K})-\\frac{1}{2}(\\mu_k+\\mu_K)^T\\mathbf{\\Sigma^{-1}}(\\mu_k-\\mu_K)+ x^T\\mathbf{\\Sigma^{-1}}(\\mu_k-\\mu_K)\\\\\n",
    "log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= a_k + \\sum_{j=1}^pb_{kj}x_j\n",
    "\n",
    "$$\n",
    "where $a_k=log(\\frac{\\pi_k}{\\pi_K})-\\frac{1}{2}(\\mu_k+\\mu_K)^T\\mathbf{\\Sigma^{-1}}(\\mu_k-\\mu_K)$ and $b_{kj}$ is the jth component of $\\mathbf{\\Sigma^{-1}}(\\mu_k-\\mu_K)$\n",
    "\n",
    "### QDA\n",
    "$$\n",
    "\n",
    "log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= a_k + \\sum_{j=1}^pb_{kj}x_j + \\sum_{j=1}^p\\sum_{l=1}^pc_{kjl}x_jx_l\n",
    "$$\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "$$\n",
    "\n",
    "log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= log(\\frac{\\pi_k}{\\pi_K})+ \\sum_{j=1}^plog(\\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\\\\\n",
    "log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= a_k + \\sum_{j=1}^pg_{kj}x_j\n",
    "\n",
    "$$\n",
    "with the right-handside takes the form of a generalized additive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "LDA is a special case of QDA with $c_{kjl}= 0$ for all $j=1,...,p,l=1,...,p$ and $k=1,...,K$\n",
    "\n",
    "Any classifier with a linear decision boundary is a special case of naive Bayes with $g_{kj}(x_j) = b_{kj}(x_j)$. Therefore, LDA is also a special case of naive Bayes.\n",
    "\n",
    "For naive Bayes if we use a one-dimensional Gaussian distribution $N(\\mu_{kj,\\sigma^2_j})$ and $\\mathbf{\\Sigma}$ restricted to be a diagonal matrix with ith diagonal element equal to $\\sigma^2_j$\n",
    "\n",
    "Neither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit, however, it is restricted to a purely additive fit. QDA, on the other hand, allow for interaction terms. Therefore, QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.\n",
    "\n",
    "Logistic regression and LDA share a similar form of linear function. However, LDA fit better when $X_1,...,X_p$ follow normal distribution within each class. and logistic regression perform better when the variables do not follow normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about KNN?\n",
    "\n",
    "- Because KNN is completely non-parametric, we can expect this ap- proach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.\n",
    "\n",
    "- Require n>>p to provide accurate classification\n",
    "\n",
    "- If n/p is not that big but the boundary is non-linear, consider QDA\n",
    "\n",
    "- KNN does not tell us which predictors are important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Generalized Linear Models\n",
    "\n",
    "$Y$ can also be neither qualitative nor quantitative but discrete counts.\n",
    "\n",
    "### 4.6.1 Linear Regression approach on count\n",
    "At first glance, linear regression provides reasonable and intuitive results.\n",
    "Some problems:\n",
    "- Some predicted data is negative which make no sense for discrete counts\n",
    "- The assumption of equal variance may not apply\n",
    "- Transform $Y$ may help but not always"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 Poisson Regression on the Bikeshare Data\n",
    "\n",
    "#### Poisson distribution:\n",
    "$$\n",
    "Pr(Y=k)=\\frac{e^{-\\lambda}\\lambda^k}{k!} \\textit{ for } k=0,1,2,...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\lambda>0$ as the expected value of $Y$ and the variance of $Y$. Meaning, if $Y$ follows the Poisson distribution, then the larger the mean of $Y$, the larger its variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we would try to estimate $\\lambda(X_1,...,X_p)$:\n",
    "$$\n",
    "log(\\lambda(X_1,...,X_p)) = \\Beta_0 + \\sum_{i=1}^p\\beta_iX_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use $\\log(\\lambda)$ instead of $\\lambda$ because we want to make sure $\\lambda$ to be positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important distinctions between the Poisson regression model and the linear regression model are as follows:\n",
    "\n",
    "- Interpretation\n",
    "\n",
    "- Mean-variance relationship:\n",
    "\n",
    "- Nonnegative fitted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 Generalized Linear Models in Greater Generality\n",
    "For the three type of regression (linear, logistic and Poisson)\n",
    "1. Each approach uses predictors to predict a response\n",
    "2. Each approach models the mean of Y as a function of the predictors but different *link funtion* $\\eta(\\mu)$\n",
    "    - For linear regression: $\\eta(\\mu)=\\mu$\n",
    "    - For Logistic regression: $\\eta(\\mu)=\\log(\\mu/(1-\\mu))$\n",
    "    - For Poisson regression: $\\eta(\\mu)=log(\\mu)$\n",
    "3. All three distributions are members of a wider class of distributions, known as the *exponential family*.\n",
    "4. In general, we can perform a regression by modeling the response $Y$ as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transfomed mean is a linear function of the predictors.\n",
    "    - Any regression approach that follows this very general recipe is known as *generalized linear model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Lab: Logistic Regression, LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import old modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import new modules\n",
    "from ISLP import confusion_table\n",
    "from ISLP.models import contrast\n",
    "from sklearn.discriminant_analysis import \\\n",
    "(LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Smarket dataset\n",
    "Smarket = load_data('Smarket')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 'Today',\n",
       "       'Direction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.030596</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.033195</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.002448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.035689</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.026155</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
       "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
       "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
       "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
       "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
       "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
       "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
       "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
       "\n",
       "           Today  \n",
       "Year    0.030095  \n",
       "Lag1   -0.026155  \n",
       "Lag2   -0.010250  \n",
       "Lag3   -0.002448  \n",
       "Lag4   -0.006900  \n",
       "Lag5   -0.034860  \n",
       "Volume  0.014592  \n",
       "Today   1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Smarket.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlations between the lagged return variables and today's return are close to zero. The only substantial correlation is between `Year` and `Volume`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Smarket.plot(y=\"Volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2 Logistic Regresion\n",
    "We will try to predict `Direction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>-0.1260</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>-0.0731</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-1.457</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>-0.0423</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef  std err      z  P>|z|\n",
       "intercept -0.1260    0.241 -0.523  0.601\n",
       "Lag1      -0.0731    0.050 -1.457  0.145\n",
       "Lag2      -0.0423    0.050 -0.845  0.398\n",
       "Lag3       0.0111    0.050  0.222  0.824\n",
       "Lag4       0.0094    0.050  0.187  0.851\n",
       "Lag5       0.0103    0.050  0.208  0.835\n",
       "Volume     0.1354    0.158  0.855  0.392"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a subset of predictors\n",
    "allvars = Smarket.columns.drop(['Today','Direction','Year'])\n",
    "# specify design\n",
    "design = MS(allvars)\n",
    "X= design.fit_transform(Smarket)\n",
    "y=Smarket.Direction ==\"Up\"\n",
    "glm=sm.GLM(y,\n",
    "           X,\n",
    "           family= sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    No strong evidence due to high p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Up', 'Down', 'Down', 'Up', 'Up', 'Up', 'Down', 'Up', 'Up', 'Down',\n",
       "       'Down', 'Up', 'Up', 'Down', 'Down', 'Up', 'Up', 'Up', 'Up', 'Down',\n",
       "       'Up', 'Up', 'Up', 'Down', 'Up', 'Up', 'Down', 'Up', 'Up', 'Up',\n",
       "       'Up', 'Up', 'Down', 'Down', 'Up', 'Up', 'Up', 'Down', 'Down',\n",
       "       'Down', 'Down', 'Up', 'Up', 'Up', 'Up', 'Up', 'Up', 'Down', 'Up',\n",
       "       'Up', 'Up', 'Down', 'Down', 'Down', 'Up', 'Up', 'Down', 'Up', 'Up',\n",
       "       'Up', 'Down', 'Down', 'Up', 'Down', 'Down', 'Down', 'Down', 'Up',\n",
       "       'Down', 'Down', 'Up', 'Up', 'Up', 'Down', 'Down', 'Down', 'Up',\n",
       "       'Down', 'Up', 'Up', 'Down', 'Down', 'Up', 'Up', 'Up', 'Up', 'Down',\n",
       "       'Down', 'Down', 'Down', 'Up', 'Down', 'Up', 'Up', 'Up', 'Up', 'Up',\n",
       "       'Up', 'Up', 'Down'], dtype='<U4')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making predictions:\n",
    "probs = results.predict()\n",
    "labels= np.array(['Down']*1250)\n",
    "labels [probs > 0.5]= \"Up\"\n",
    "labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>145</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Up</th>\n",
       "      <td>457</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      Down   Up\n",
       "Predicted           \n",
       "Down        145  141\n",
       "Up          457  507"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate confusion_table\n",
    "confusion_table(labels, Smarket.Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the model is working a bit better than random guessing. Remember though, this is the training test error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using train and test data\n",
    "train = Smarket.Year < 2005\n",
    "Smarket_train = Smarket.loc[train] \n",
    "Smarket_test = Smarket.loc[~ train] \n",
    "Smarket_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test= X.loc[train], X.loc[~train]\n",
    "y_train, y_test= y.loc[train], y.loc[~train]\n",
    "glm_train = sm.GLM(y_train,\n",
    "                   X_train,\n",
    "                   family=sm.families.Binomial())\n",
    "results = glm_train.fit()\n",
    "probs = results.predict(exog=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Smarket.Direction\n",
    "L_train, L_test = D.loc[train], D.loc[~train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>77</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Up</th>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      Down  Up\n",
       "Predicted          \n",
       "Down         77  97\n",
       "Up           34  44"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([\"Down\"]*252)\n",
    "labels[probs>0.5]=\"Up\"\n",
    "confusion_table(labels,L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that logistic regression model had very undershelming p-values associated with all of the predictors. Perhaps by removing the variables , we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Up</th>\n",
       "      <td>76</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      Down   Up\n",
       "Predicted           \n",
       "Down         35   35\n",
       "Up           76  106"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MS(['Lag1', 'Lag2']).fit(Smarket)\n",
    "X = model.transform(Smarket)\n",
    "X_train, X_test = X.loc[train], X.loc[~train] \n",
    "glm_train = sm.GLM(y_train,\n",
    "X_train ,\n",
    "family=sm.families.Binomial()) \n",
    "results = glm_train.fit()\n",
    "probs = results.predict(exog=X_test) \n",
    "labels = np.array(['Down']*252) \n",
    "labels[probs>0.5] = 'Up' \n",
    "confusion_table(labels, L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 Linear Discriminant Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearDiscriminantAnalysis(store_covariance=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(store_covariance=True)\n",
    "# since LDA automatically add a intercep column, \\\n",
    "    # remove the column from X_train and X_test\n",
    "X_train, X_test = [x.drop(columns=\"intercept\") for x in [X_train,X_test]]\n",
    "lda.fit(X_train,L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04279022,  0.03389409],\n",
       "       [-0.03954635, -0.03132544]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the means\n",
    "lda.means_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `means_` attribute show the mean of predictors within each class. These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `sklearn` typically uses the trailing `_` to denote a quantity estimated when using the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Down', 'Up'], dtype='<U4')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49198397, 0.50801603])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.64201904],\n",
       "       [-0.51352928]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.scalings_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values provide the linear combination of `Lag1` and `Lag2` that are used to form the LDA decision rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Up</th>\n",
       "      <td>76</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      Down   Up\n",
       "Predicted           \n",
       "Down         35   35\n",
       "Up           76  106"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_pred= lda.predict(X_test)\n",
    "confusion_table(lda_pred,L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_prob = lda.predict_proba(X_test)\n",
    "np.all(np.where(lda_prob[:,1] >= 0.5, 'Up','Down') == lda_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.where(lda_prob[:,1] >= 0.5, 'Up','Down') == lda_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuadraticDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "QuadraticDiscriminantAnalysis(store_covariance=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda=QDA(store_covariance=True)\n",
    "qda.fit(X_train, L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.04279022,  0.03389409],\n",
       "        [-0.03954635, -0.03132544]]),\n",
       " array([0.49198397, 0.50801603]),\n",
       " array(['Down', 'Up'], dtype=object))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda.means_, qda.priors_, qda.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.50662277, -0.03924806],\n",
       "        [-0.03924806,  1.53559498]]),\n",
       " array([[ 1.51700576, -0.02787349],\n",
       "        [-0.02787349,  1.49026815]])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# covariaces per class\n",
    "qda.covariance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Up</th>\n",
       "      <td>81</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      Down   Up\n",
       "Predicted           \n",
       "Down         30   20\n",
       "Up           81  121"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda_pred = qda.predict(X_test)\n",
    "confusion_table(qda_pred, L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5992063492063492"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(qda_pred==L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.5 Naive Bayes\n",
    "This use the `GaussianNB()` in which each quantitative feature using a Gaussian distribution. However, a kernal density method can also be used to estimate the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB=GaussianNB()\n",
    "NB.fit(X_train,L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Down', 'Up'], dtype='<U4'),\n",
       " array([0.49198397, 0.50801603]),\n",
       " array([[ 0.04279022,  0.03389409],\n",
       "        [-0.03954635, -0.03132544]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.classes_, NB.class_prior_, NB.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.50355429, 1.53246749],\n",
       "       [1.51401364, 1.48732877]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Up</th>\n",
       "      <td>82</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      Down   Up\n",
       "Predicted           \n",
       "Down         29   20\n",
       "Up           82  121"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_labels = NB.predict(X_test)\n",
    "confusion_table(nb_labels, L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5952380952380952"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(nb_labels==L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.6 K-Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>43</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Up</th>\n",
       "      <td>68</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      Down  Up\n",
       "Predicted          \n",
       "Down         43  58\n",
       "Up           68  83"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiating the model\n",
    "knn1=KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(X_train,L_train)\n",
    "knn1_pred = knn1.predict(X_test)\n",
    "confusion_table(knn1_pred,L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5317460317460317"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeate the analysis with k=3\n",
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn3_pred = knn3.fit(X_train, L_train).predict(X_test)\n",
    "np.mean(knn3_pred == L_test)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, \n",
    "\n",
    "    the scale of the variables matter in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to handle this problem is to standardize the data so that all variables are given a mean of zero, and a standard deviation of one.\n",
    "`StandardScaler()` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     5474\n",
       "Yes     348\n",
       "Name: Purchase, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data\n",
    "Caravan = load_data('Caravan') \n",
    "Purchase = Caravan.Purchase \n",
    "Purchase.value_counts()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature processing\n",
    "feature_df = Caravan.drop(columns=['Purchase'])\n",
    "scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
    "X_std= scaler.fit_transform(feature_df)\n",
    "feature_std = pd.DataFrame(X_std,columns = feature_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data set\n",
    "(X_train,\n",
    "X_test ,\n",
    "y_train ,\n",
    "y_test) = train_test_split(feature_std,\n",
    "Purchase , test_size=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.111, 0.067)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1_pred = knn1.fit(X_train, y_train).predict(X_test)\n",
    "np.mean(y_test != knn1_pred), np.mean(y_test != \"No\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this may appear to be fairly good. However, since just over 6% of customers purchased insurance, we could get the error rate down to almost 6% by always predicting `No` regardless of the values of the predictors! This is known as the *null rate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>880</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth       No  Yes\n",
       "Predicted          \n",
       "No         880   58\n",
       "Yes         53    9"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_table(knn1_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning parameters\n",
    "The number of neighbors in KNN is referred to as a *tuning parameter*, also referred to as a *hyperparameter*. We do not know a priori what value to use. It is therefore of interest to see how the classifier performs on test data as we vary these parameters. This can be achieved with a *for* loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1: # predicted to rent: 62, # who did rent 9, accuracy 14.5%\n",
      "K=2: # predicted to rent:  6, # who did rent 1, accuracy 16.7%\n",
      "K=3: # predicted to rent: 20, # who did rent 3, accuracy 15.0%\n",
      "K=4: # predicted to rent:  4, # who did rent 0, accuracy 0.0%\n",
      "K=5: # predicted to rent:  7, # who did rent 1, accuracy 14.3%\n"
     ]
    }
   ],
   "source": [
    "for K in range(1,6):\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    knn_pred = knn.fit(X_train, y_train).predict(X_test) \n",
    "    C = confusion_table(knn_pred, y_test)\n",
    "    templ = ('K={0:d}: # predicted to rent: {1:>2},' +\n",
    "    ' # who did rent {2:d}, accuracy {3:.1%}') \n",
    "    pred = C.loc['Yes'].sum()\n",
    "    did_rent = C.loc['Yes','Yes'] \n",
    "    print(templ.format(\n",
    "    K,\n",
    "    pred , did_rent ,\n",
    "    did_rent / pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison to Logistic Regression\n",
    "\n",
    "    Note that sklearn fit ridge regression version of logistic regression\n",
    "\n",
    "This can be modified by appropriately setting the argument `C`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>933</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth       No  Yes\n",
       "Predicted          \n",
       "No         933   67\n",
       "Yes          0    0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=1e10, solver='liblinear') \n",
    "logit.fit(X_train, y_train)\n",
    "logit_pred = logit.predict_proba(X_test)\n",
    "logit_labels = np.where(logit_pred[:,1] > 5, 'Yes', 'No') \n",
    "confusion_table(logit_labels, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results use a threshold of 0.5 Let's change that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>913</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth       No  Yes\n",
       "Predicted          \n",
       "No         913   58\n",
       "Yes         20    9"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_labels = np.where(logit_pred[:,1]>0.25, 'Yes', 'No') \n",
    "confusion_table(logit_labels, y_test)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much better result 9/29=0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.7 Linear and Poisson Regression on the Bikeshare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8645, 15),\n",
       " Index(['season', 'mnth', 'day', 'hr', 'holiday', 'weekday', 'workingday',\n",
       "        'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual',\n",
       "        'registered', 'bikers'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "Bike = load_data('Bikeshare')\n",
    "Bike.shape, Bike.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>-68.6317</td>\n",
       "      <td>5.307</td>\n",
       "      <td>-12.932</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Feb]</th>\n",
       "      <td>6.8452</td>\n",
       "      <td>4.287</td>\n",
       "      <td>1.597</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[March]</th>\n",
       "      <td>16.5514</td>\n",
       "      <td>4.301</td>\n",
       "      <td>3.848</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[April]</th>\n",
       "      <td>41.4249</td>\n",
       "      <td>4.972</td>\n",
       "      <td>8.331</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[May]</th>\n",
       "      <td>72.5571</td>\n",
       "      <td>5.641</td>\n",
       "      <td>12.862</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[June]</th>\n",
       "      <td>67.8187</td>\n",
       "      <td>6.544</td>\n",
       "      <td>10.364</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[July]</th>\n",
       "      <td>45.3245</td>\n",
       "      <td>7.081</td>\n",
       "      <td>6.401</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Aug]</th>\n",
       "      <td>53.2430</td>\n",
       "      <td>6.640</td>\n",
       "      <td>8.019</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Sept]</th>\n",
       "      <td>66.6783</td>\n",
       "      <td>5.925</td>\n",
       "      <td>11.254</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Oct]</th>\n",
       "      <td>75.8343</td>\n",
       "      <td>4.950</td>\n",
       "      <td>15.319</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Nov]</th>\n",
       "      <td>60.3100</td>\n",
       "      <td>4.610</td>\n",
       "      <td>13.083</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Dec]</th>\n",
       "      <td>46.4577</td>\n",
       "      <td>4.271</td>\n",
       "      <td>10.878</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[1]</th>\n",
       "      <td>-14.5793</td>\n",
       "      <td>5.699</td>\n",
       "      <td>-2.558</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[2]</th>\n",
       "      <td>-21.5791</td>\n",
       "      <td>5.733</td>\n",
       "      <td>-3.764</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[3]</th>\n",
       "      <td>-31.1408</td>\n",
       "      <td>5.778</td>\n",
       "      <td>-5.389</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[4]</th>\n",
       "      <td>-36.9075</td>\n",
       "      <td>5.802</td>\n",
       "      <td>-6.361</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[5]</th>\n",
       "      <td>-24.1355</td>\n",
       "      <td>5.737</td>\n",
       "      <td>-4.207</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[6]</th>\n",
       "      <td>20.5997</td>\n",
       "      <td>5.704</td>\n",
       "      <td>3.612</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[7]</th>\n",
       "      <td>120.0931</td>\n",
       "      <td>5.693</td>\n",
       "      <td>21.095</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[8]</th>\n",
       "      <td>223.6619</td>\n",
       "      <td>5.690</td>\n",
       "      <td>39.310</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[9]</th>\n",
       "      <td>120.5819</td>\n",
       "      <td>5.693</td>\n",
       "      <td>21.182</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[10]</th>\n",
       "      <td>83.8013</td>\n",
       "      <td>5.705</td>\n",
       "      <td>14.689</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[11]</th>\n",
       "      <td>105.4234</td>\n",
       "      <td>5.722</td>\n",
       "      <td>18.424</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[12]</th>\n",
       "      <td>137.2837</td>\n",
       "      <td>5.740</td>\n",
       "      <td>23.916</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[13]</th>\n",
       "      <td>136.0359</td>\n",
       "      <td>5.760</td>\n",
       "      <td>23.617</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[14]</th>\n",
       "      <td>126.6361</td>\n",
       "      <td>5.776</td>\n",
       "      <td>21.923</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[15]</th>\n",
       "      <td>132.0865</td>\n",
       "      <td>5.780</td>\n",
       "      <td>22.852</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[16]</th>\n",
       "      <td>178.5206</td>\n",
       "      <td>5.772</td>\n",
       "      <td>30.927</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[17]</th>\n",
       "      <td>296.2670</td>\n",
       "      <td>5.749</td>\n",
       "      <td>51.537</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[18]</th>\n",
       "      <td>269.4409</td>\n",
       "      <td>5.736</td>\n",
       "      <td>46.976</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[19]</th>\n",
       "      <td>186.2558</td>\n",
       "      <td>5.714</td>\n",
       "      <td>32.596</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[20]</th>\n",
       "      <td>125.5492</td>\n",
       "      <td>5.704</td>\n",
       "      <td>22.012</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[21]</th>\n",
       "      <td>87.5537</td>\n",
       "      <td>5.693</td>\n",
       "      <td>15.378</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[22]</th>\n",
       "      <td>59.1226</td>\n",
       "      <td>5.689</td>\n",
       "      <td>10.392</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[23]</th>\n",
       "      <td>26.8376</td>\n",
       "      <td>5.688</td>\n",
       "      <td>4.719</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workingday</th>\n",
       "      <td>1.2696</td>\n",
       "      <td>1.784</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>157.2094</td>\n",
       "      <td>10.261</td>\n",
       "      <td>15.321</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit[cloudy/misty]</th>\n",
       "      <td>-12.8903</td>\n",
       "      <td>1.964</td>\n",
       "      <td>-6.562</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit[heavy rain/snow]</th>\n",
       "      <td>-109.7446</td>\n",
       "      <td>76.667</td>\n",
       "      <td>-1.431</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit[light rain/snow]</th>\n",
       "      <td>-66.4944</td>\n",
       "      <td>2.965</td>\n",
       "      <td>-22.425</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 coef  std err       t  P>|t|\n",
       "intercept                    -68.6317    5.307 -12.932  0.000\n",
       "mnth[Feb]                      6.8452    4.287   1.597  0.110\n",
       "mnth[March]                   16.5514    4.301   3.848  0.000\n",
       "mnth[April]                   41.4249    4.972   8.331  0.000\n",
       "mnth[May]                     72.5571    5.641  12.862  0.000\n",
       "mnth[June]                    67.8187    6.544  10.364  0.000\n",
       "mnth[July]                    45.3245    7.081   6.401  0.000\n",
       "mnth[Aug]                     53.2430    6.640   8.019  0.000\n",
       "mnth[Sept]                    66.6783    5.925  11.254  0.000\n",
       "mnth[Oct]                     75.8343    4.950  15.319  0.000\n",
       "mnth[Nov]                     60.3100    4.610  13.083  0.000\n",
       "mnth[Dec]                     46.4577    4.271  10.878  0.000\n",
       "hr[1]                        -14.5793    5.699  -2.558  0.011\n",
       "hr[2]                        -21.5791    5.733  -3.764  0.000\n",
       "hr[3]                        -31.1408    5.778  -5.389  0.000\n",
       "hr[4]                        -36.9075    5.802  -6.361  0.000\n",
       "hr[5]                        -24.1355    5.737  -4.207  0.000\n",
       "hr[6]                         20.5997    5.704   3.612  0.000\n",
       "hr[7]                        120.0931    5.693  21.095  0.000\n",
       "hr[8]                        223.6619    5.690  39.310  0.000\n",
       "hr[9]                        120.5819    5.693  21.182  0.000\n",
       "hr[10]                        83.8013    5.705  14.689  0.000\n",
       "hr[11]                       105.4234    5.722  18.424  0.000\n",
       "hr[12]                       137.2837    5.740  23.916  0.000\n",
       "hr[13]                       136.0359    5.760  23.617  0.000\n",
       "hr[14]                       126.6361    5.776  21.923  0.000\n",
       "hr[15]                       132.0865    5.780  22.852  0.000\n",
       "hr[16]                       178.5206    5.772  30.927  0.000\n",
       "hr[17]                       296.2670    5.749  51.537  0.000\n",
       "hr[18]                       269.4409    5.736  46.976  0.000\n",
       "hr[19]                       186.2558    5.714  32.596  0.000\n",
       "hr[20]                       125.5492    5.704  22.012  0.000\n",
       "hr[21]                        87.5537    5.693  15.378  0.000\n",
       "hr[22]                        59.1226    5.689  10.392  0.000\n",
       "hr[23]                        26.8376    5.688   4.719  0.000\n",
       "workingday                     1.2696    1.784   0.711  0.477\n",
       "temp                         157.2094   10.261  15.321  0.000\n",
       "weathersit[cloudy/misty]     -12.8903    1.964  -6.562  0.000\n",
       "weathersit[heavy rain/snow] -109.7446   76.667  -1.431  0.152\n",
       "weathersit[light rain/snow]  -66.4944    2.965 -22.425  0.000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = MS(['mnth', 'hr',\n",
    "    'workingday', 'temp',\n",
    "    'weathersit']).fit_transform(Bike) \n",
    "Y = Bike['bikers']\n",
    "M_lm = sm.OLS(Y, X).fit() \n",
    "summarize(M_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>73.5974</td>\n",
       "      <td>5.132</td>\n",
       "      <td>14.340</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Jan]</th>\n",
       "      <td>-46.0871</td>\n",
       "      <td>4.085</td>\n",
       "      <td>-11.281</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Feb]</th>\n",
       "      <td>-39.2419</td>\n",
       "      <td>3.539</td>\n",
       "      <td>-11.088</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[March]</th>\n",
       "      <td>-29.5357</td>\n",
       "      <td>3.155</td>\n",
       "      <td>-9.361</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[April]</th>\n",
       "      <td>-4.6622</td>\n",
       "      <td>2.741</td>\n",
       "      <td>-1.701</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[May]</th>\n",
       "      <td>26.4700</td>\n",
       "      <td>2.851</td>\n",
       "      <td>9.285</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[June]</th>\n",
       "      <td>21.7317</td>\n",
       "      <td>3.465</td>\n",
       "      <td>6.272</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[July]</th>\n",
       "      <td>-0.7626</td>\n",
       "      <td>3.908</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Aug]</th>\n",
       "      <td>7.1560</td>\n",
       "      <td>3.535</td>\n",
       "      <td>2.024</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Sept]</th>\n",
       "      <td>20.5912</td>\n",
       "      <td>3.046</td>\n",
       "      <td>6.761</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Oct]</th>\n",
       "      <td>29.7472</td>\n",
       "      <td>2.700</td>\n",
       "      <td>11.019</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth[Nov]</th>\n",
       "      <td>14.2229</td>\n",
       "      <td>2.860</td>\n",
       "      <td>4.972</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[0]</th>\n",
       "      <td>-96.1420</td>\n",
       "      <td>3.955</td>\n",
       "      <td>-24.307</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[1]</th>\n",
       "      <td>-110.7213</td>\n",
       "      <td>3.966</td>\n",
       "      <td>-27.916</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[2]</th>\n",
       "      <td>-117.7212</td>\n",
       "      <td>4.016</td>\n",
       "      <td>-29.310</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[3]</th>\n",
       "      <td>-127.2828</td>\n",
       "      <td>4.081</td>\n",
       "      <td>-31.191</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[4]</th>\n",
       "      <td>-133.0495</td>\n",
       "      <td>4.117</td>\n",
       "      <td>-32.319</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[5]</th>\n",
       "      <td>-120.2775</td>\n",
       "      <td>4.037</td>\n",
       "      <td>-29.794</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[6]</th>\n",
       "      <td>-75.5424</td>\n",
       "      <td>3.992</td>\n",
       "      <td>-18.925</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[7]</th>\n",
       "      <td>23.9511</td>\n",
       "      <td>3.969</td>\n",
       "      <td>6.035</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[8]</th>\n",
       "      <td>127.5199</td>\n",
       "      <td>3.950</td>\n",
       "      <td>32.284</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[9]</th>\n",
       "      <td>24.4399</td>\n",
       "      <td>3.936</td>\n",
       "      <td>6.209</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[10]</th>\n",
       "      <td>-12.3407</td>\n",
       "      <td>3.936</td>\n",
       "      <td>-3.135</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[11]</th>\n",
       "      <td>9.2814</td>\n",
       "      <td>3.945</td>\n",
       "      <td>2.353</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[12]</th>\n",
       "      <td>41.1417</td>\n",
       "      <td>3.957</td>\n",
       "      <td>10.397</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[13]</th>\n",
       "      <td>39.8939</td>\n",
       "      <td>3.975</td>\n",
       "      <td>10.036</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[14]</th>\n",
       "      <td>30.4940</td>\n",
       "      <td>3.991</td>\n",
       "      <td>7.641</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[15]</th>\n",
       "      <td>35.9445</td>\n",
       "      <td>3.995</td>\n",
       "      <td>8.998</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[16]</th>\n",
       "      <td>82.3786</td>\n",
       "      <td>3.988</td>\n",
       "      <td>20.655</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[17]</th>\n",
       "      <td>200.1249</td>\n",
       "      <td>3.964</td>\n",
       "      <td>50.488</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[18]</th>\n",
       "      <td>173.2989</td>\n",
       "      <td>3.956</td>\n",
       "      <td>43.806</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[19]</th>\n",
       "      <td>90.1138</td>\n",
       "      <td>3.940</td>\n",
       "      <td>22.872</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[20]</th>\n",
       "      <td>29.4071</td>\n",
       "      <td>3.936</td>\n",
       "      <td>7.471</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[21]</th>\n",
       "      <td>-8.5883</td>\n",
       "      <td>3.933</td>\n",
       "      <td>-2.184</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr[22]</th>\n",
       "      <td>-37.0194</td>\n",
       "      <td>3.934</td>\n",
       "      <td>-9.409</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workingday</th>\n",
       "      <td>1.2696</td>\n",
       "      <td>1.784</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>157.2094</td>\n",
       "      <td>10.261</td>\n",
       "      <td>15.321</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit[cloudy/misty]</th>\n",
       "      <td>-12.8903</td>\n",
       "      <td>1.964</td>\n",
       "      <td>-6.562</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit[heavy rain/snow]</th>\n",
       "      <td>-109.7446</td>\n",
       "      <td>76.667</td>\n",
       "      <td>-1.431</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit[light rain/snow]</th>\n",
       "      <td>-66.4944</td>\n",
       "      <td>2.965</td>\n",
       "      <td>-22.425</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 coef  std err       t  P>|t|\n",
       "intercept                     73.5974    5.132  14.340  0.000\n",
       "mnth[Jan]                    -46.0871    4.085 -11.281  0.000\n",
       "mnth[Feb]                    -39.2419    3.539 -11.088  0.000\n",
       "mnth[March]                  -29.5357    3.155  -9.361  0.000\n",
       "mnth[April]                   -4.6622    2.741  -1.701  0.089\n",
       "mnth[May]                     26.4700    2.851   9.285  0.000\n",
       "mnth[June]                    21.7317    3.465   6.272  0.000\n",
       "mnth[July]                    -0.7626    3.908  -0.195  0.845\n",
       "mnth[Aug]                      7.1560    3.535   2.024  0.043\n",
       "mnth[Sept]                    20.5912    3.046   6.761  0.000\n",
       "mnth[Oct]                     29.7472    2.700  11.019  0.000\n",
       "mnth[Nov]                     14.2229    2.860   4.972  0.000\n",
       "hr[0]                        -96.1420    3.955 -24.307  0.000\n",
       "hr[1]                       -110.7213    3.966 -27.916  0.000\n",
       "hr[2]                       -117.7212    4.016 -29.310  0.000\n",
       "hr[3]                       -127.2828    4.081 -31.191  0.000\n",
       "hr[4]                       -133.0495    4.117 -32.319  0.000\n",
       "hr[5]                       -120.2775    4.037 -29.794  0.000\n",
       "hr[6]                        -75.5424    3.992 -18.925  0.000\n",
       "hr[7]                         23.9511    3.969   6.035  0.000\n",
       "hr[8]                        127.5199    3.950  32.284  0.000\n",
       "hr[9]                         24.4399    3.936   6.209  0.000\n",
       "hr[10]                       -12.3407    3.936  -3.135  0.002\n",
       "hr[11]                         9.2814    3.945   2.353  0.019\n",
       "hr[12]                        41.1417    3.957  10.397  0.000\n",
       "hr[13]                        39.8939    3.975  10.036  0.000\n",
       "hr[14]                        30.4940    3.991   7.641  0.000\n",
       "hr[15]                        35.9445    3.995   8.998  0.000\n",
       "hr[16]                        82.3786    3.988  20.655  0.000\n",
       "hr[17]                       200.1249    3.964  50.488  0.000\n",
       "hr[18]                       173.2989    3.956  43.806  0.000\n",
       "hr[19]                        90.1138    3.940  22.872  0.000\n",
       "hr[20]                        29.4071    3.936   7.471  0.000\n",
       "hr[21]                        -8.5883    3.933  -2.184  0.029\n",
       "hr[22]                       -37.0194    3.934  -9.409  0.000\n",
       "workingday                     1.2696    1.784   0.711  0.477\n",
       "temp                         157.2094   10.261  15.321  0.000\n",
       "weathersit[cloudy/misty]     -12.8903    1.964  -6.562  0.000\n",
       "weathersit[heavy rain/snow] -109.7446   76.667  -1.431  0.152\n",
       "weathersit[light rain/snow]  -66.4944    2.965 -22.425  0.000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_encode = contrast('hr', 'sum')\n",
    "mnth_encode = contrast('mnth', 'sum')\n",
    "X2 = MS([mnth_encode, hr_encode ,\n",
    "'workingday', 'temp','weathersit']).fit_transform(Bike) \n",
    "M2_lm = sm.OLS(Y, X2).fit()\n",
    "S2 = summarize(M2_lm) \n",
    "S2\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(idx='hr', name='hr', is_categorical=True, is_ordinal=False, columns=(), encoder=Contrast(method='sum'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ML_2, the coefficients of `hr` and `mnth` will always sum to zero, and can be interpreted as the difference from the mean level\n",
    "As long as you interpret the model output correctly in light of the coding used, the coding really does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_pois = sm.GLM(Y, X2, family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_pois = summarize(M_pois)\n",
    "coef_month = S_pois[S_pois.index.str.contains('mnth')]['coef'] \n",
    "coef_month = pd.concat([coef_month,\n",
    "pd.Series([-coef_month.sum()], index=['mnth[Dec]'])]) \n",
    "coef_hr = S_pois[S_pois.index.str.contains('hr')]['coef']\n",
    "coef_hr = pd.concat([coef_hr, pd.Series([-coef_hr.sum()],\n",
    "index=['hr[23]'])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('skl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "824b1118aeb34277907cf67e6aa2d279c243d5f30303de89e2239204acf1e71f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
