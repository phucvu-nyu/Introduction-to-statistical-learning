{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Statistical Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What is statistical learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we observe a quantitative response $Y$ and p different predictors, $X1 , X2 , . . . , Xp$ . We assume that there is some relationship between $Y$ and $X = (X1,X2,...,Xp)$, which can be written in the very general form\n",
    "$$\n",
    "Y=f(X)+\\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $f$ is some fixed but unknown function of $X1, . . . , Xp$, and $\\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formula- tion, $f$ represents the systematic information that $X$ provides about $Y$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistical learning refers to a set of approaches for estimating $f$. In this chapter we outline some of the key theoretical concepts that arise in estimating $f$, as well as tools for evaluating the estimates obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Why estimate $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two main reasons: prediction and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a set of inputs $X$ but the output $Y$ cannot be easily obtained, since the error term averages to zero, we can predict Y using\n",
    "$$\n",
    "\\hat{Y}=\\hat{f}(X)\n",
    "$$\n",
    "where $\\hat{f}$ represents our estimate for $f$ ans is usually treated as a black box , and $\\hat{Y}$ represents the resulting pre- diction for $Y$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the reducible error and the irreducible error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, $\\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is *reducible* because we can potentially improve the accuracy of $\\hat{f}$ using the most appropriate statistical learning technique to estimate $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if it were possible to form a perfect estimate for $f$, so that our estimated response took the form $\\hat{Y} = f(X)$, our prediction would still have some error in it! This is because $Y$ is also a function of $\\epsilon$, which, by definition, cannot be predicted using $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, variability associated with $\\epsilon$ also affects the accuracy of our predictions. This is known as the *irreducible error*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(Y-\\hat{Y})^2 = E[f(X)+\\epsilon -\\hat{f}(X)]^2=[f(X)-\\hat{f}(X)]^2_{reducible} + Var(\\epsilon)_{irreducible}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "We are often interested in understanding the association between $Y$ and $X1,...,Xp$. In this situation we wish to estimate $f$, but our goal is not necessarily to make predictions for $Y$. Now $\\hat{f}$ cannot be treated as a black box because we need to know its exact form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer foolowing questions:\n",
    "- Which predictors are associated with the response? \n",
    "- What is the relationship between the response and each predictor?\n",
    "- Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 How Do We Estimate $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have observed a set of $n$ different data points aka training data. Then our training data consist of ${(x_1,y_1),...(x_n,y_n)}$ where $x_i=(x_{i1},...,x_{ip})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to apply a statistical method to the training data in order to estimate the unknown function $f$. \n",
    "Broadly speaking, most statistical learning methods for this task can be characterized as either *parametrix* or *non-parametric*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametric methods\n",
    "involve a two-step model-based approach\n",
    "1. First, we make an assumption about the functional form, or shape of $f$\n",
    "2. After a model has bene selected, we need a procedure that uses the training data to *fit* or *train* the model.\n",
    "\n",
    "$\\Rightarrow$ The most common approach to fitting the model is referred to as (ordinary) least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-based approach just described is referred to as *parametric*; it reduces the problem of estimating $f$ down to own of estimating a set of parameters\n",
    "\n",
    "The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of $f$.\n",
    "\n",
    "$\\Rightarrow$ if the chosen model is too far from the true $f$, then our estimate will be poor \n",
    "$\\Rightarrow$ we can try to address this problem by choosing *flexible* models that can fit many different possible functional forms for $f$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, fitting a more flexible model requires estimating a greater number of parameters aka more complex models and can lead to *overfitting* the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-parametric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do not make explicit assumptions about the func- tional form of $f$.\n",
    "- Seek an estimate of $f$ that gets as close to the data pointsas possible without being too rough or wiggly\n",
    "- Avoid the danger that $\\hat{f}$ is very different of $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may ask: *why would we ever\n",
    "choose to use a more restrictive method instead of a very flexible approach?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the reasons that we might prefer a more restrictive model:\n",
    "- If we are mainly interested in inference, then restrictive models are much more interpreable\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Supervised Versus Unsupervised Learning\n",
    "- supervised: what we have talked about so far\n",
    "- unsupervised: we obser a vector of measurements $x_i$, but no associated respons $y_i$\n",
    "    - One statistical learning tool that we may use in this setting is cluster analysis or clustering\n",
    "- Sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut.\n",
    "    - For m of the observa- tions, where m < n, we have both predictor measurements and a response measurement. For the remaining n âˆ’ m observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a semi-supervised learning problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Regression Versus Classification Problems\n",
    "- Problems with a quantitative response: regression problems\n",
    "- Problesm with a qualitative response: classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Assessing Model Accuracy\n",
    "### 2.2.1 Measuring the Quality of Fit\n",
    "In regression setting, the most commonly-used measure is the *mean squared error* (MSE) given by\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum^n_{i=1}(y_i - \\hat{f}(x_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE.\n",
    "\n",
    "But in general, *we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the pre- dictions that we obtain when we apply our method to previously unseen test data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ We want to choose the method that gives the lowest test MSE\n",
    "$$\n",
    "testMSE =Ave(y_0-\\hat{f}(x_0))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flexibility ~ degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 The Bias-Variance Trade-Off\n",
    "We have:\n",
    "\n",
    "$$\n",
    "E(y_0-\\hat{f}(x_0))^2 = Var(\\hat{f}(x_0))+[Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\n",
    "$$\n",
    "We have $E(y_0-\\hat{f}(x_0))^2$ defines the *expected test MSE* at $x_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variance refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set\n",
    "    - Ideally the estimate for $f$ should not vary too much between training sets.\n",
    "    - If a method has high variance then small changes in the training data can result in large changes in $\\hat{f}$\n",
    "    - In general, more flexible statistical methods have higher variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model\n",
    "    - Generally, more flexible methods result in less bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.\n",
    "    - The relative rate of change of these two quantities determines whhether the test MSE increases or decreases\n",
    "$\\Rightarrow$ The relationship between bias, variance and test set MSE given above is referred to as the *bias-variance* trade-off\n",
    "$\\Rightarrow$ The challende lies in finding a method for which both the variance and the squared bias are low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 The Classification Setting\n",
    "The moset common approach for quantifying the accuracy of our estimate $\\hat{f}$ is the training *error rate*\n",
    "$$\n",
    "\\textit{error rate} = \\frac{1}{n}\\sum_{i=1}^nI(y_i\\neq \\hat{y_i})\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow \\textit{test error rate} = Ave(I(y_i\\neq \\hat{y_i}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bayes Classifier\n",
    "It is shown that the test error rate given is minimized, on average, by a very simple classifier that assigns each observation to the most likely class given its predictor values\n",
    "aka Assign a test observation with predictor vector $x_0$ to the class $j$ for which \n",
    "$$\n",
    "Pr(Y=j|X=x_0)\n",
    "$$\n",
    "is largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes classifier produces the lowest possible test error rate, called the *Bayes error rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Lab: Introduction to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "x.ndim\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('skl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "824b1118aeb34277907cf67e6aa2d279c243d5f30303de89e2239204acf1e71f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
